{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, add_remaining_self_loops, degree\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"model/gcn.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb\n",
    "%run temporal_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run graph.ipynb\n",
    "data_ns, adj_matrix_ns, conv_layer_ns = create_graph_pyg_ns(df_filtered, num_neighbors=4, hidden_channels=64)\n",
    "data_sw, adj_matrix_sw, conv_layer_sw = create_graph_pyg_sw(df_filtered, hidden_channels=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Convolution Layer\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        support = torch.matmul(x, self.weight)\n",
    "        output = torch.sparse.mm(adj, support)\n",
    "        output = output + self.bias\n",
    "        return F.relu(output)\n",
    "\n",
    "# GCN Model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(input_dim, hidden_dim1)\n",
    "        self.gc2 = GraphConvolution(hidden_dim1, hidden_dim2)\n",
    "        self.gc3 = GraphConvolution(hidden_dim2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim1)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim2)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        x = self.gc2(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        x = self.gc3(x, adj)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return sqrt(((predictions - targets) ** 2).mean().item())\n",
    "\n",
    "\n",
    "def train_gcn_model(model, data, adj_matrix, epochs=150, lr=0.001, accumulation_steps=10, clip_value=1, stop_loss=0.6, save_path=None):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    mean = data.x.mean(dim=0)\n",
    "    std = data.x.std(dim=0)\n",
    "    data.x = (data.x - mean) / std\n",
    "\n",
    "    losses = []\n",
    "    rmses = []\n",
    "    r2_scores = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        accumulated_loss = 0.0\n",
    "\n",
    "        for i in range(accumulation_steps):\n",
    "            output = model(data.x, adj_matrix)\n",
    "            loss = criterion(output.view(-1), data.y)\n",
    "            loss.backward()\n",
    "            accumulated_loss += loss.item() / accumulation_steps\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(accumulated_loss)\n",
    "\n",
    "        if accumulated_loss <= stop_loss:\n",
    "            print(f\"Early stopping as accumulated loss went below stop loss\")\n",
    "            print(f\"Epoch {epoch + 1}, Loss : {accumulated_loss}\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            return (f\"Final model saved at {save_path}\")\n",
    "            break\n",
    "\n",
    "        rmse_val = rmse(output.view(-1), data.y)\n",
    "        rmses.append(rmse_val)\n",
    "        r2_val = r2_score(data.y.cpu().numpy(), output.view(-1).detach().cpu().numpy())\n",
    "        r2_scores.append(r2_val)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {accumulated_loss}, RMSE: {rmse_val}, R^2: {r2_val}')\n",
    "\n",
    "    # Save the final model if a save path is provided\n",
    "\n",
    "\n",
    "    return rmses, losses, r2_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2161.0853515625, RMSE: 46.29673789213582, R^2: -2142.388117733131\n",
      "Epoch 2, Loss: 143.7158889770508, RMSE: 12.317000287965817, R^2: -150.70850607725723\n",
      "Epoch 3, Loss: 8.280826902389526, RMSE: 2.988314918876912, R^2: -7.930026088711404\n",
      "Epoch 4, Loss: 1.5730021238327025, RMSE: 1.2343697849598103, R^2: -0.5236689892241178\n",
      "Epoch 5, Loss: 1.3959659457206726, RMSE: 1.1823894555518581, R^2: -0.39804468299060813\n",
      "Epoch 6, Loss: 1.290188539028168, RMSE: 1.1367455443275434, R^2: -0.29219026788739644\n",
      "Epoch 7, Loss: 1.1994545936584473, RMSE: 1.0964869451271868, R^2: -0.20228375958692535\n",
      "Epoch 8, Loss: 1.1311429977416991, RMSE: 1.0637731777763568, R^2: -0.13161360914369724\n",
      "Epoch 9, Loss: 1.0817867159843444, RMSE: 1.0394990620574462, R^2: -0.0805582665186495\n",
      "Epoch 10, Loss: 1.045251190662384, RMSE: 1.0223514531058302, R^2: -0.04520260366522111\n",
      "Epoch 11, Loss: 1.0214882612228395, RMSE: 1.0105184261297413, R^2: -0.02114746036856885\n",
      "Epoch 12, Loss: 1.0019840061664582, RMSE: 1.0006935575541451, R^2: -0.001387730070631088\n",
      "Epoch 13, Loss: 0.9870304226875306, RMSE: 0.9945711354111099, R^2: 0.01082832280718371\n",
      "Epoch 14, Loss: 0.9715017855167388, RMSE: 0.9872658983227043, R^2: 0.02530608537278356\n",
      "Epoch 15, Loss: 1.0694439649581908, RMSE: 0.9852455165270273, R^2: 0.029291229347187087\n",
      "Epoch 16, Loss: 1.200966686010361, RMSE: 0.9829332298758006, R^2: 0.03384219643568431\n",
      "Epoch 17, Loss: 1.1921820282936098, RMSE: 0.9695355706499025, R^2: 0.06000078426033206\n",
      "Epoch 18, Loss: 0.9472987771034241, RMSE: 0.9716271774442305, R^2: 0.05594067542859349\n",
      "Epoch 19, Loss: 0.9509717524051666, RMSE: 0.9680342645157034, R^2: 0.06290961088196134\n",
      "Epoch 20, Loss: 0.9368458390235901, RMSE: 0.9671122335257203, R^2: 0.06469395941968814\n",
      "Epoch 21, Loss: 0.9350951015949249, RMSE: 0.9683731946001399, R^2: 0.06225327223662325\n",
      "Epoch 22, Loss: 0.9298785746097564, RMSE: 0.9648957335024441, R^2: 0.06897600774023582\n",
      "Epoch 23, Loss: 0.923964011669159, RMSE: 0.961823372995269, R^2: 0.0748957831482483\n",
      "Epoch 24, Loss: 0.9169303715229035, RMSE: 0.9585405554153865, R^2: 0.08120003503655027\n",
      "Epoch 25, Loss: 0.9107332527637482, RMSE: 0.9525501518863314, R^2: 0.09264823136244271\n",
      "Epoch 26, Loss: 0.9030520498752592, RMSE: 0.9500661450498, R^2: 0.09737425761462848\n",
      "Epoch 27, Loss: 0.9118253886699677, RMSE: 0.9474404085286909, R^2: 0.10235670014272846\n",
      "Epoch 28, Loss: 0.893024241924286, RMSE: 0.9432719883220013, R^2: 0.11023805859734215\n",
      "Epoch 29, Loss: 0.9080794811248779, RMSE: 0.9441027821921976, R^2: 0.1086700585735868\n",
      "Epoch 30, Loss: 0.8844158053398132, RMSE: 0.9428914175110047, R^2: 0.1109558023985514\n",
      "Epoch 31, Loss: 0.8820983171463013, RMSE: 0.9399325917809106, R^2: 0.11652668343071004\n",
      "Epoch 32, Loss: 0.882153993844986, RMSE: 0.938073586790264, R^2: 0.1200178624354169\n",
      "Epoch 33, Loss: 0.8759706020355225, RMSE: 0.9348528682997914, R^2: 0.1260500704022307\n",
      "Epoch 34, Loss: 0.8715676486492157, RMSE: 0.9321418549491034, R^2: 0.13111151955625666\n",
      "Epoch 35, Loss: 0.8690572202205659, RMSE: 0.9314589733194673, R^2: 0.13238419470771057\n",
      "Epoch 36, Loss: 0.8653942525386811, RMSE: 0.9301800104480628, R^2: 0.13476527039901143\n",
      "Epoch 37, Loss: 0.8616334021091462, RMSE: 0.9296787485944951, R^2: 0.1356974956291015\n",
      "Epoch 38, Loss: 0.8574442982673645, RMSE: 0.926839232008635, R^2: 0.14096897596128277\n",
      "Epoch 39, Loss: 0.853701114654541, RMSE: 0.924070330558575, R^2: 0.14609394969171718\n",
      "Epoch 40, Loss: 0.8490866780281068, RMSE: 0.92269295434516, R^2: 0.1486377990990051\n",
      "Epoch 41, Loss: 0.8473891973495483, RMSE: 0.9212466215798946, R^2: 0.15130470106268967\n",
      "Epoch 42, Loss: 1.082336938381195, RMSE: 0.9244862121886659, R^2: 0.14532535166928318\n",
      "Epoch 43, Loss: 0.8440428912639617, RMSE: 0.9157872677904552, R^2: 0.1613338141635754\n",
      "Epoch 44, Loss: 0.8376127362251282, RMSE: 0.9132877228885211, R^2: 0.16590547962405788\n",
      "Epoch 45, Loss: 0.8353126585483551, RMSE: 0.9121876587039383, R^2: 0.16791364450654733\n",
      "Epoch 46, Loss: 0.8305864870548247, RMSE: 0.9139332476140478, R^2: 0.16472595067483475\n",
      "Epoch 47, Loss: 0.8285792171955108, RMSE: 0.9078257114939483, R^2: 0.17585262506742072\n",
      "Epoch 48, Loss: 0.8245419025421142, RMSE: 0.9090933098002727, R^2: 0.1735493080623851\n",
      "Epoch 49, Loss: 0.8209792494773864, RMSE: 0.9054329576239468, R^2: 0.18019111908442065\n",
      "Epoch 50, Loss: 0.8173655509948728, RMSE: 0.9035440033951415, R^2: 0.1836081890433383\n",
      "Epoch 51, Loss: 0.8140474975109101, RMSE: 0.9014822740742734, R^2: 0.18732978223762098\n",
      "Epoch 52, Loss: 0.81082843542099, RMSE: 0.9019464716411909, R^2: 0.18649256058906827\n",
      "Epoch 53, Loss: 0.809205949306488, RMSE: 0.8981278258978042, R^2: 0.1933664184682382\n",
      "Epoch 54, Loss: 0.8035500943660736, RMSE: 0.8937228685542431, R^2: 0.20125952800784375\n",
      "Epoch 55, Loss: 0.8275903224945068, RMSE: 0.8928387551457861, R^2: 0.2028389547526529\n",
      "Epoch 56, Loss: 0.7974456369876861, RMSE: 0.8950077436953935, R^2: 0.1989610120768831\n",
      "Epoch 57, Loss: 0.7933595478534697, RMSE: 0.8899562399532223, R^2: 0.20797792309678598\n",
      "Epoch 58, Loss: 0.7902368545532227, RMSE: 0.8888656237325909, R^2: 0.20991789221598123\n",
      "Epoch 59, Loss: 0.785979849100113, RMSE: 0.8869797242738349, R^2: 0.21326691537874143\n",
      "Epoch 60, Loss: 0.7837567150592804, RMSE: 0.8859796342468093, R^2: 0.2150400271575965\n",
      "Epoch 61, Loss: 0.7788420617580414, RMSE: 0.8824126426750808, R^2: 0.2213479415050169\n",
      "Epoch 62, Loss: 0.9123660624027252, RMSE: 0.8790214971641217, R^2: 0.22732122578800817\n",
      "Epoch 63, Loss: 0.7739787518978118, RMSE: 0.8787413378813584, R^2: 0.2278136053398251\n",
      "Epoch 64, Loss: 0.7697686195373535, RMSE: 0.8729069062068913, R^2: 0.23803353911148128\n",
      "Epoch 65, Loss: 0.7717692017555235, RMSE: 0.875748586598529, R^2: 0.23306440104292148\n",
      "Epoch 66, Loss: 0.7640154361724852, RMSE: 0.8749177757866805, R^2: 0.234518888486146\n",
      "Epoch 67, Loss: 0.7595867753028869, RMSE: 0.869467718745896, R^2: 0.24402589673634822\n",
      "Epoch 68, Loss: 0.7582281589508058, RMSE: 0.871640876044449, R^2: 0.24024204648074954\n",
      "Epoch 69, Loss: 0.7545378625392913, RMSE: 0.8682434938221617, R^2: 0.2461532465478078\n",
      "Epoch 70, Loss: 0.7516648173332215, RMSE: 0.8656925340329424, R^2: 0.2505764663042944\n",
      "Epoch 71, Loss: 0.7494813740253449, RMSE: 0.8654919449769106, R^2: 0.25092372502066274\n",
      "Epoch 72, Loss: 0.745437228679657, RMSE: 0.8625379443460482, R^2: 0.25602833150760185\n",
      "Epoch 73, Loss: 0.7427985429763795, RMSE: 0.8607894008525468, R^2: 0.25904161430900374\n",
      "Epoch 74, Loss: 0.7382416069507598, RMSE: 0.8586786743594562, R^2: 0.2626709266608839\n",
      "Epoch 75, Loss: 0.735435974597931, RMSE: 0.8589407083567597, R^2: 0.26222093381116096\n",
      "Epoch 76, Loss: 0.7320810198783875, RMSE: 0.8547790394636836, R^2: 0.2693528483108615\n",
      "Epoch 77, Loss: 0.759740161895752, RMSE: 0.8522261387200226, R^2: 0.2737104955461356\n",
      "Epoch 78, Loss: 0.8261325001716613, RMSE: 0.8537459073729602, R^2: 0.2711179489203124\n",
      "Epoch 79, Loss: 0.7236644566059113, RMSE: 0.8544327197498894, R^2: 0.2699448101728257\n",
      "Epoch 80, Loss: 0.7265988945960998, RMSE: 0.8489989795588818, R^2: 0.27920069342643805\n",
      "Epoch 81, Loss: 0.7191049754619597, RMSE: 0.8473881240303348, R^2: 0.28193330110320325\n",
      "Epoch 82, Loss: 0.7167978286743163, RMSE: 0.8456046595527151, R^2: 0.28495276627148103\n",
      "Epoch 83, Loss: 0.7119765400886536, RMSE: 0.843266949308836, R^2: 0.2889008110388597\n",
      "Epoch 84, Loss: 0.7126348078250886, RMSE: 0.8409990747562358, R^2: 0.2927205261216226\n",
      "Epoch 85, Loss: 0.708817058801651, RMSE: 0.8432020598105908, R^2: 0.28901031792260856\n",
      "Epoch 86, Loss: 0.7049147188663483, RMSE: 0.8418379417368316, R^2: 0.2913089021108025\n",
      "Epoch 87, Loss: 0.7034010231494904, RMSE: 0.8388132559833512, R^2: 0.2963922959084656\n",
      "Epoch 88, Loss: 0.7003716945648194, RMSE: 0.8355885784625845, R^2: 0.301791720986168\n",
      "Epoch 89, Loss: 0.6965659856796266, RMSE: 0.8368554823681317, R^2: 0.29967285359582085\n"
     ]
    }
   ],
   "source": [
    "gcn_model = GCN(input_dim=data_ns.x.size(1), hidden_dim1=256, hidden_dim2=256, output_dim=1)\n",
    "rmses, losses, r2_scores = train_gcn_model(gcn_model, data_ns, adj_matrix_ns, epochs=200, lr=0.001, accumulation_steps=10, clip_value=1, stop_loss=0.4, save_path=file_path)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title('Training Loss Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rmses, label='RMSE')\n",
    "plt.title('RMSE Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(r2_scores, label='R^2')\n",
    "plt.title('R^2 Score Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model = GCN(input_dim=data_sw.x.size(1), hidden_dim=512, output_dim=1)\n",
    "rmses, losses, r2_scores = train_gcn_model(gcn_model, data_sw, adj_matrix_sw, epochs=200, lr=0.001, accumulation_steps=10, clip_value=1, stop_loss=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title('Training Loss Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rmses, label='RMSE')\n",
    "plt.title('RMSE Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(r2_scores, label='R^2')\n",
    "plt.title('R^2 Score Per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('R2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
