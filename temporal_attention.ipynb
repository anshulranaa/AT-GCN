{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path  = \"model/attn.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run temporal_preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_50_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data, date_encodings):\n",
    "        self.data = data\n",
    "        self.date_encodings = date_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gid = list(self.data.keys())[idx]\n",
    "        features = np.array(self.data[gid][:, :-2], dtype=np.float32)  # Exclude the last two columns if last is target and second last is date\n",
    "        target = np.array(self.data[gid][:, -1], dtype=np.float32)\n",
    "        dates = self.date_encodings[gid]\n",
    "        return torch.tensor(features), torch.tensor(dates), torch.tensor(target)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x, dates):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TemporalTransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, d_model, nhead, num_layers, dim_feedforward=512, dropout=0.1):\n",
    "        super(TemporalTransformerEncoder, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_size, d_model // 2)\n",
    "        self.pos_encoder = PositionalEncoding(d_model // 2, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model // 2, nhead=nhead // 2, dim_feedforward=dim_feedforward // 2, dropout=dropout, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output_layer = nn.Linear(d_model // 2, 1)\n",
    "\n",
    "    def forward(self, x, dates):\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.pos_encoder(x, dates)\n",
    "        x = self.transformer_encoder(x)\n",
    "        output = self.output_layer(x)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper DataLoader initialization\n",
    "dataset = TransformerDataset(transformer_input, date_encodings)\n",
    "train_set, val_set = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader([dataset[i] for i in train_set], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader([dataset[i] for i in val_set], batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "model = TemporalTransformerEncoder(input_size=len(top_50_features), d_model=12, nhead=4, num_layers=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional metrics for performance evaluation\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = ((y_true - y_pred) ** 2).sum()\n",
    "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "\n",
    "# Lists to store loss and metrics for visualization\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_r2_scores = []\n",
    "\n",
    "# Adjust the training loop to include validation metrics\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, dates, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, dates)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_r2 = 0\n",
    "    with torch.no_grad():\n",
    "        for x, dates, target in val_loader:\n",
    "            output = model(x, dates)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            total_r2 += r_squared(target, output)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    avg_r2 = total_r2 / len(val_loader)\n",
    "    val_r2_scores.append(avg_r2)\n",
    "\n",
    "    # Save the model if the training loss meets certain condition\n",
    "    if train_loss <= 0.4:\n",
    "        print(f\"Early Stoping at Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val R^2: {avg_r2:.4f}\") \n",
    "        torch.save(model.state_dict(), file_path)\n",
    "        print(f\"Model saved at {file_path}\")\n",
    "        break\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val R^2: {avg_r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting R-squared values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_r2_scores, label='Validation R^2 Score')\n",
    "plt.title('R-squared Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
